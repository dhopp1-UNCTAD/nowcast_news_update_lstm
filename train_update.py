import datetimefrom dateutil.relativedelta import relativedeltaimport pandas as pdimport numpy as npimport torchimport dillimport osfrom nowcast_lstm.LSTM import LSTM### news functiondef add_last_dates(data, target_period):    dataset = data.copy()    last_date = datetime.datetime.strptime(        datetime.datetime.strftime(np.max(dataset.date), "%Y-%m-%d"), "%Y-%m-%d"    )    period_date = datetime.datetime.strptime(target_period, "%Y-%m-%d")    months_add = (period_date.year - last_date.year) * 12 + (        period_date.month - last_date.month    )    if months_add > 0:        for i in range(months_add):            dataset.loc[len(dataset), "date"] = dataset.loc[                len(dataset) - 1, "date"            ] + relativedelta(months=1)    return datasetdef gen_news(model, target_period, old_data, new_data):    """Generate the news between two data releases using the method of holding out new data feature by feature and recording the differences in model output		parameters:		:model: LSTM.LSTM: trained LSTM model		:target_period: str: target prediction date        :old_data: pd.DataFrame: previous dataset        :new_data: pd.DataFrame: new dataset		output: Dict		:news: dataframe of news contribution of each column with updated data. scaled_news is news scaled to sum to actual prediction delta.		:old_pred: prediction on the previous dataset        :new_pred: prediction on the new dataset        :holdout_discrepency: % difference between the sum of news via the holdout method and the actual prediction delta	"""    data = {}    data["old"] = old_data.copy()    data["new"] = new_data.copy()    data["vintage"] = new_data.copy()    # force all dataframes to have same number of rows    data["old"] = data["new"].loc[:, ["date"]].merge(data["old"], on="date", how="left")    # vintage data is new data, but with missings where old had missings. For revisions contribution.    data["vintage"] = data["new"].copy()    for col in data["vintage"].columns:        data["vintage"].loc[pd.isna(data["old"][col]), col] = np.nan    # make sure datasets have the target period. Can remove if library can handle future dates    for dataset in ["old", "new", "vintage"]:        data[dataset] = add_last_dates(data[dataset], target_period)    # predictions on each dataframe, subsequently getting revisions contribution    preds = pd.DataFrame(columns=["column", "prediction"])    for dataset in ["old", "new", "vintage"]:        preds = preds.append(            pd.DataFrame(                {                    "column": dataset,                    "prediction": model.predict(data[dataset])                    .loc[lambda x: x.date == target_period]                    .predictions.values[0],                },                index=[0],            )        ).reset_index(drop=True)    # looping through each column    for col in data["vintage"].columns:        if col != target:            # any new values?            if not all(                list(                    (data["vintage"][col] == data["new"][col])                    | (pd.isna(data["vintage"][col]) & pd.isna(data["new"][col]))                )            ):                # predictions on new - new value (subtractive method)                subtractive = data["new"].copy()                subtractive[col] = data["vintage"][col]                preds = preds.append(                    pd.DataFrame(                        {                            "column": col,                            "prediction": model.predict(subtractive)                            .loc[lambda x: x.date == target_period]                            .predictions.values[0],                        },                        index=[0],                    )                ).reset_index(drop=True)    # scale the news so it adds to actual delta    old_pred = preds.loc[preds.column == "old", "prediction"].values[0]    new_pred = preds.loc[preds.column == "new", "prediction"].values[0]    revisions = preds.loc[preds.column == "vintage", "prediction"].values[0] - old_pred    delta = new_pred - old_pred    subtractive = (        preds.loc[preds.column == "new", "prediction"].values[0]        - preds.loc[~preds.column.isin(["new", "old", "vintage"]), "prediction"]    )    subtractive = pd.DataFrame(        {            "column": preds.loc[subtractive.index, "column"].values,            "news": subtractive.values,        }    )    news = subtractive.copy()    news.loc[len(news), "news"] = revisions    news.loc[len(news) - 1, "column"] = "revisions"    if delta != 0:        diff = news.news.sum() / delta        news["scaled_news"] = news.news / diff    else:        diff = 1        news["scaled_news"] = news.news    return {"news":news, "old_pred":old_pred, "new_pred":new_pred, "holdout_discrepency":diff}### news function### data informationdata_directory = "/Users/danhopp/dhopp1/UNCTAD/nowcast_data_update/"unctad_web_directory = "/Users/danhopp/dhopp1/UNCTAD/unctad-nowcast-web/"retrain = Falsetargets = ["x_world", "x_vol_world2", "x_servs_world"]target_periods = ["2020-06-01", "2020-09-01", "2020-12-01", "2021-03-01", "2021-06-01", "2021-09-01", "2021-12-01"]# final model parameters and variables from selectionvariables_dict = {}variables_dict["x_world"] = ['x_world', 'bci_nl', 'constr_ca', 'manuf_ba_conf_de', 'export_orders_fr', 'x_vol_world2', 'manuf_ba_conf_pl', 'fc_gdp_uk', 'x_vol_afr', 'fc_x_us', 'ipi_de', 'ipi_it', 'serv_emp_fut_nl', 'x_servs_us', 'manuf_ba_conf_it', 'fc_gdp_oecd', 'p_manuf', 'container_hk', 'x_ru', 'ipi_jp', 'fc_x_de', 'bci_br', 'manuf_emp_fut_it', 'x_in', 'x_us']variables_dict["x_vol_world2"] = ['x_vol_world2', 'x_servs_ez', 'ipi_jp', 'manuf_shipments_us', 'ipi_fr', 'rti_vol_oecd', 'serv_conf_fr', 'air_freight_hkg', 'fc_gdp_jp', 'bci_jp', 'manuf_emp_fut_it', 'manuf_orders_de', 'bci_kr', 'bci_oecd', 'ipi_uk', 'ipi_it', 'x_vol_world', 'manuf_ba_conf_nl', 'rti_vol_us', 'manuf_emp_fut_de', 'constr_ca', 'export_orders_nl', 'x_vol_afr']variables_dict["x_servs_world"] = ['x_servs_us', 'fc_gdp_us', 'manuf_emp_fut_it', 'x_servs_world', 'p_manuf', 'x_servs_sg', 'manuf_orders_us_2', 'ipi_eu27', 'fc_x_de', 'export_orders_uk', 'export_orders_it', 'cci_br', 'rti_val_es', 'rti_val_fr', 'ipi_jp', 'manuf_orders_it', 'x_world', 'fc_x_us', 'x_vol_ez', 'x_nl', 'bci_nl']if retrain:    ### training, use 2020-03-01 dataset    train = pd.read_csv(data_directory + "output/2020-03-01_database_tf.csv", parse_dates=["date"])    train = train.loc[train.date <= "2019-12-01",:]        # parameters for model_selection    n_timesteps_dict = {}; n_timesteps_dict["x_world"] = 9; n_timesteps_dict["x_vol_world2"] = 7; n_timesteps_dict["x_servs_world"] = 7;     train_episodes_dict = {}; train_episodes_dict["x_world"] = 200; train_episodes_dict["x_vol_world2"] = 200; train_episodes_dict["x_servs_world"] = 200;     batch_size_dict = {}; batch_size_dict["x_world"] = 40; batch_size_dict["x_vol_world2"] = 45; batch_size_dict["x_servs_world"] = 45;     n_hidden_dict = {}; n_hidden_dict["x_world"] = 40; n_hidden_dict["x_vol_world2"] = 50; n_hidden_dict["x_servs_world"] = 50;     n_layers_dict = {}; n_layers_dict["x_world"] = 1; n_layers_dict["x_vol_world2"] = 1; n_layers_dict["x_servs_world"] = 4;     for target in targets:        train_i = train.loc[:, ["date"] + variables_dict[target]]        # train lstm        model = LSTM(            data=train_i,             target_variable=target,             n_timesteps=n_timesteps_dict[target],            fill_na_func=np.nanmean,            fill_ragged_edges_func=np.nanmean,            n_models=10,            train_episodes=train_episodes_dict[target],            batch_size=batch_size_dict[target],            lr=0.01,            decay=0.98,            n_hidden=n_hidden_dict[target],            n_layers=n_layers_dict[target],            dropout=0,            criterion=torch.nn.MSELoss()        )                model.train(quiet=True)                dill.dump(model, open("trained_models/" + target + ".pkl", mode="wb"))                # generating updatefiles = os.listdir(f"{data_directory}/output/")files = [x[:10] for x in files if "_tf" in x]  files.sort()result = pd.DataFrame(columns=["date_forecast", "series", "value", "target", "target_period"])for target in targets:    model = dill.load(open("trained_models/" + target + ".pkl", "rb", -1))    for target_period in target_periods:        print(f"{target}: {target_period}")        for i in range(1, len(files)):            # only do if target date within 93 days of file date            if (np.abs((datetime.datetime.strptime(files[i], "%Y-%m-%d") - datetime.datetime.strptime(target_period, "%Y-%m-%d")).days) <= 93):                data_i = pd.read_csv(data_directory + "output/" + files[i] + "_database_tf.csv", parse_dates=["date"])                data_i = data_i.loc[:, ["date"] + variables_dict[target]]                # for news                data_i_old = pd.read_csv(data_directory + "output/" + files[i-1] + "_database_tf.csv", parse_dates=["date"])                data_i_old = data_i_old.loc[:, ["date"] + variables_dict[target]]                # force all dataframes to have same number of rows                data_i_old = data_i.loc[:, ["date"]].merge(data_i_old, on="date", how="left")                                # make sure target date is in                data_i = add_last_dates(data_i, target_period)                data_i_old = add_last_dates(data_i_old, target_period)                                # news                news = gen_news(model, target_period, data_i_old, data_i)                tmp = news["news"].copy().drop("news", axis=1)                tmp.columns = ["series", "value"]                tmp.loc[tmp.series == "revisions", "series"] = "impact_revisions"                tmp.loc[len(tmp), "series"] = "forecast"                tmp.loc[len(tmp)-1, "value"] = news["new_pred"]                tmp["date_forecast"] = files[i]                tmp["target"] = target                tmp["target_period"] = target_period                                result = result.append(tmp, ignore_index=True).reset_index(drop=True)result.to_csv(f"{unctad_web_directory}/nowcasts/data/lstm.csv", index=False)print('\033[92m' + "LSTM successfully updated")